{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 01 : Generating Tattoos from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kovarthanan/Acedamic/City, University of London/INM716 - Industrial AI/INM716_Coursework/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check device availability of GPU\n",
    "\"\"\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # if you are using colab\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\" # If running in mac\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load dataset from Hugging Face\n",
    "\"\"\"\n",
    "dataset = load_dataset(\"Drozdik/tattoo_v0\")\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define image preprocessing\n",
    "\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Function to preprocess dataset for DataLoader\n",
    "\"\"\"\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [transform(sample[\"image\"]) for sample in batch]\n",
    "    captions = [\"Tattoo of \" + sample[\"text\"]\n",
    "                for sample in batch]  # Modify captions here\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack(images),  # Convert to tensor\n",
    "        \"text\": captions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "DataLoader with collate_fn\n",
    "\"\"\" \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 11.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Load pre-trained Stable Diffusion model\n",
    "\"\"\"\n",
    "\n",
    "# model_id = \"prompthero/openjourney\"\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Apply LoRa for easy finetune with minimal computational power\n",
    "\"\"\"\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"to_q\", \"to_v\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 797,184 || all params: 860,318,148 || trainable%: 0.0927\n"
     ]
    }
   ],
   "source": [
    "pipeline.unet = get_peft_model(pipeline.unet, lora_config)\n",
    "pipeline.unet.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Setting up optimizer\n",
    "\"\"\"\n",
    "optimizer = AdamW(pipeline.unet.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Tokenizer and Text Encoder\n",
    "\"\"\" \n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder=\"text_encoder\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(captions):\n",
    "    inputs = tokenizer(\n",
    "        captions,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return text_encoder(inputs.input_ids.to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "    model_id, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Encode images into latent space using VAE\n",
    "\"\"\" \n",
    "\n",
    "vae = pipeline.vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images_to_latent_space(images):\n",
    "    # Ensure input images are in float32\n",
    "    images = images.to(device)\n",
    "    # Move images to latent space (4 channels)\n",
    "    with torch.no_grad():  # No gradients for VAE encoding\n",
    "        latents = vae.encode(images).latent_dist.sample()\n",
    "        latents = latents * 0.18215  # Scaling factor used in Stable Diffusion\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  42%|████▏     | 1850/4370 [16:33<22:58,  1.83it/s, loss=0.00119] "
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Training loop\n",
    "\"\"\"\n",
    "# Training loop with progress bars\n",
    "pipeline.unet.train()\n",
    "for epoch in range(5): \n",
    "    optimizer.zero_grad()\n",
    "    #dataloader with tqdm\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        captions = batch[\"text\"]\n",
    "        # Encode images\n",
    "        latents = encode_images_to_latent_space(images)\n",
    "        # Encode captions\n",
    "        encoder_hidden_states = encode_captions(captions)\n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        bsz = latents.shape[0]\n",
    "        timesteps = torch.randint(\n",
    "            0, noise_scheduler.config.num_train_timesteps, (bsz,), device=device).long()\n",
    "        # Add noise\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "        # Predict the noise residual\n",
    "        noise_pred = pipeline.unet(\n",
    "            noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss = loss / accumulation_steps  # Normalize loss\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Gradient Accumulation\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Update progress\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item() * accumulation_steps}\")\n",
    "\n",
    "    # Save LoRA weights as .pth after each epoch\n",
    "    lora_state_dict = {name: param for name,\n",
    "                       param in pipeline.unet.named_parameters() if param.requires_grad}\n",
    "    torch.save(lora_state_dict,\n",
    "               f\"Project_01/training_weights/lora_finetuned_weights_epoch_{epoch+1}.pth\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Generate tattoo from text\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"A elephant head drawn with sacred geometric patterns\"\n",
    "# prompt = \" A skull with digital glitch distortions and neon streaks.\"\n",
    "# prompt = \"A futuristic mask with neon symbols floating around it.\"\n",
    "# prompt = \"Tatoo of A delicate, face mask with celestial engravings and soft glowing edges.\"\n",
    "\n",
    "image = pipeline(prompt).images[0]\n",
    "\n",
    "# Save or display the image\n",
    "image.save(\"finetuned_generated_tattoo.png\")\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
